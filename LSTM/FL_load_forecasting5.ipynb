{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minsoo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/minsoo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/minsoo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/minsoo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/minsoo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/minsoo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/home/minsoo/anaconda3/lib/python3.6/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.13.1.so', error was \"/lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.25' not found (required by /home/minsoo/anaconda3/lib/python3.6/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.13.1.so)\".\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from syft.frameworks.torch.nn import rnn\n",
    "from syft.federated.floptimizer import Optims\n",
    "from syft.frameworks.torch.fl import utils\n",
    "\n",
    "from functools import wraps\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import syft as sy\n",
    "\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.autograd import Variable\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 32\n",
    "        self.test_batch_size = 16\n",
    "        self.seed = 1\n",
    "        self.epochs = 50\n",
    "        self.learning_rate = 1e-3\n",
    "        self.worker_epochs = 1\n",
    "        self.federate_after_n_batches = 1\n",
    "        # hyper parameters\n",
    "        self.hidden_dim = 64\n",
    "        self.output_dim = 24\n",
    "\n",
    "        self.look_ahead = 1*24 # a day-ahead forecasting\n",
    "        self.look_back = 7*24 # using a week load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset to input\n",
    "def build_dataset(time_series, look_back, look_ahead):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    print(len(time_series) - look_back)\n",
    "    for i in range(0, len(time_series) - look_back, look_ahead):\n",
    "        sample_X = time_series[i : (i + look_back), :]\n",
    "        sample_Y = time_series[i + look_back : i + look_back + look_ahead, :]\n",
    "        dataX.append(sample_X)\n",
    "        dataY.append(sample_Y)\n",
    "        \n",
    "    #print(np.asarray(dataX).shape)\n",
    "    #print(np.asarray(dataY).shape)\n",
    "    data_numX = np.array(dataX, dtype = 'float32')\n",
    "    data_numY = np.array(dataY, dtype = 'float32')\n",
    "    return data_numX, data_numY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time_Series_Data(Dataset):\n",
    "\n",
    "    def __init__(self, train_x, train_y):\n",
    "        self.X = train_x\n",
    "        self.y = train_y\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        x_t = self.X[item]\n",
    "        y_t = self.y[item]\n",
    "        return x_t, y_t\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "             with_scaling=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"weather_from_14y_8_1_to_16y.csv\", index_col = 0)\n",
    "\n",
    "scaler_temp = RobustScaler()\n",
    "scaler_temp.fit(df['Temperature?'].values.reshape(-1,1))\n",
    "\n",
    "scaler_ws = RobustScaler()\n",
    "scaler_ws.fit(df['Wind_speed(m/s)'].values.reshape(-1,1))\n",
    "\n",
    "scaler_wd = RobustScaler()\n",
    "scaler_wd.fit(df['Wind_direction'].values.reshape(-1,1))\n",
    "\n",
    "scaler_hum = RobustScaler()\n",
    "scaler_hum.fit(df['Humidity(%)'].values.reshape(-1,1))\n",
    "\n",
    "scaler_vp = RobustScaler()\n",
    "scaler_vp.fit(df['Vapor_pressure(hPa)'].values.reshape(-1,1))\n",
    "\n",
    "scaler_lp = RobustScaler()\n",
    "scaler_lp.fit(df['Local_pressure(hPa)'].values.reshape(-1,1))\n",
    "\n",
    "scaler_sr = RobustScaler()\n",
    "scaler_sr.fit(df['Solar_radiation(MJ/m2)'].values.reshape(-1,1))\n",
    "\n",
    "scaler_month = RobustScaler()\n",
    "scaler_month.fit(df['month'].values.reshape(-1,1))\n",
    "\n",
    "scaler_day = RobustScaler()\n",
    "scaler_day.fit(df['day'].values.reshape(-1,1))\n",
    "\n",
    "scaler_hour = RobustScaler()\n",
    "scaler_hour.fit(df['hour'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(dataframe, train_start, train_end, valid_start, valid_end, test_start, test_end):\n",
    "    data_train = dataframe[train_start:train_end].values.reshape(-1,1)\n",
    "    data_valid = dataframe[valid_start:valid_end].values.reshape(-1,1)\n",
    "    data_test = dataframe[test_start:test_end].values.reshape(-1,1)\n",
    "    return data_train, data_valid, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19536\n",
      "744\n",
      "744\n",
      "dataTrainX.shape (814, 168, 10)\n",
      "dataTrainY.shape (814, 24, 10)\n"
     ]
    }
   ],
   "source": [
    "#train_start =\"2014-08-01 8:00\"\n",
    "#train_end = \"2016-01-30 7:00\"\n",
    "\n",
    "#train_start =\"2014-08-01 8:00\"\n",
    "#train_end = \"2016-08-31 7:00\"\n",
    "\n",
    "train_start =\"2014-08-01 8:00\"\n",
    "train_end = \"2016-10-30 7:00\"\n",
    "\n",
    "valid_start = \"2016-11-23 8:00\"\n",
    "valid_end = \"2016-12-31 7:00\"\n",
    "\n",
    "test_start = \"2016-11-23 8:00\"\n",
    "test_end = \"2016-12-31 7:00\"\n",
    "\n",
    "\n",
    "\n",
    "temp_train, temp_valid, temp_test = data_preprocess(df['Temperature?'], train_start, train_end, valid_start, valid_end, test_start, test_end)\n",
    "\n",
    "ws_train, ws_valid, ws_test = data_preprocess(df['Wind_speed(m/s)'], train_start, train_end, valid_start, valid_end, test_start, test_end)\n",
    "\n",
    "wd_train, wd_valid, wd_test = data_preprocess(df['Wind_direction'], train_start, train_end, valid_start, valid_end, test_start, test_end)\n",
    "\n",
    "hum_train, hum_valid, hum_test = data_preprocess(df['Humidity(%)'], train_start, train_end, valid_start, valid_end, test_start, test_end)\n",
    "\n",
    "vp_train, vp_valid, vp_test = data_preprocess(df['Vapor_pressure(hPa)'], train_start, train_end, valid_start, valid_end, test_start, test_end)\n",
    "\n",
    "lp_train, lp_valid, lp_test = data_preprocess(df['Local_pressure(hPa)'], train_start, train_end, valid_start, valid_end, test_start, test_end)\n",
    "\n",
    "sr_train, sr_valid, sr_test = data_preprocess(df['Solar_radiation(MJ/m2)'], train_start, train_end, valid_start, valid_end, test_start, test_end)\n",
    "\n",
    "month_train, month_valid, month_test = data_preprocess(df['month'], train_start, train_end, valid_start, valid_end, test_start, test_end)\n",
    "\n",
    "day_train, day_valid, day_test = data_preprocess(df['day'], train_start, train_end, valid_start, valid_end, test_start, test_end)\n",
    "\n",
    "hour_train, hour_valid, hour_test = data_preprocess(df['hour'], train_start, train_end, valid_start, valid_end, test_start, test_end)\n",
    "\n",
    "temp_train_scaled = scaler_temp.transform(temp_train)\n",
    "temp_valid_scaled = scaler_temp.transform(temp_valid)\n",
    "temp_test_scaled = scaler_temp.transform(temp_test)\n",
    "\n",
    "ws_train_scaled = scaler_ws.transform(ws_train)\n",
    "ws_valid_scaled = scaler_ws.transform(ws_valid)\n",
    "ws_test_scaled = scaler_ws.transform(ws_test)\n",
    "\n",
    "wd_train_scaled = scaler_wd.transform(wd_train)\n",
    "wd_valid_scaled = scaler_wd.transform(wd_valid)\n",
    "wd_test_scaled = scaler_wd.transform(wd_test)\n",
    "\n",
    "hum_train_scaled = scaler_hum.transform(hum_train)\n",
    "hum_valid_scaled = scaler_hum.transform(hum_valid)\n",
    "hum_test_scaled = scaler_hum.transform(hum_test)\n",
    "\n",
    "vp_train_scaled = scaler_vp.transform(vp_train)\n",
    "vp_valid_scaled = scaler_vp.transform(vp_valid)\n",
    "vp_test_scaled = scaler_vp.transform(vp_test)\n",
    "\n",
    "lp_train_scaled = scaler_lp.transform(lp_train)\n",
    "lp_valid_scaled = scaler_lp.transform(lp_valid)\n",
    "lp_test_scaled = scaler_lp.transform(lp_test)\n",
    "\n",
    "sr_train_scaled = scaler_sr.transform(sr_train)\n",
    "sr_valid_scaled = scaler_sr.transform(sr_valid)\n",
    "sr_test_scaled = scaler_sr.transform(sr_test)\n",
    "\n",
    "month_train_scaled = scaler_month.transform(month_train)\n",
    "month_valid_scaled = scaler_month.transform(month_valid)\n",
    "month_test_scaled = scaler_month.transform(month_test)\n",
    "\n",
    "day_train_scaled = scaler_day.transform(day_train)\n",
    "day_valid_scaled = scaler_day.transform(day_valid)\n",
    "day_test_scaled = scaler_day.transform(day_test)\n",
    "\n",
    "hour_train_scaled = scaler_hour.transform(hour_train)\n",
    "hour_valid_scaled = scaler_hour.transform(hour_valid)\n",
    "hour_test_scaled = scaler_hour.transform(hour_test)\n",
    "\n",
    "data_train = np.hstack((temp_train_scaled, ws_train_scaled, \n",
    "                        wd_train_scaled, hum_train_scaled, vp_train_scaled, \n",
    "                        lp_train_scaled, sr_train_scaled, month_train_scaled, day_train_scaled, \n",
    "                        day_train_scaled))\n",
    "\n",
    "data_valid = np.hstack((temp_valid_scaled, ws_valid_scaled, \n",
    "                        wd_valid_scaled, hum_valid_scaled, vp_valid_scaled, \n",
    "                        lp_valid_scaled, sr_valid_scaled,month_valid_scaled, day_valid_scaled, \n",
    "                        day_valid_scaled))\n",
    "\n",
    "data_test = np.hstack((temp_test_scaled, ws_test_scaled, \n",
    "                        wd_test_scaled, hum_test_scaled, vp_test_scaled, \n",
    "                        lp_test_scaled, sr_test_scaled,month_test_scaled, day_test_scaled, \n",
    "                        day_test_scaled))\n",
    "\n",
    "dataTrainX, dataTrainY = build_dataset(data_train, args.look_back, args.look_ahead)\n",
    "dataValidX, dataValidY = build_dataset(data_valid, args.look_back, args.look_ahead)\n",
    "dataTestX, dataTestY = build_dataset(data_test, args.look_back, args.look_ahead)\n",
    "\n",
    "print(\"dataTrainX.shape\", dataTrainX.shape)\n",
    "print(\"dataTrainY.shape\", dataTrainY.shape)\n",
    "\n",
    "dataset_train = Time_Series_Data(dataTrainX, dataTrainY)\n",
    "dataset_valid = Time_Series_Data(dataValidX, dataValidY)\n",
    "dataset_test = Time_Series_Data(dataTestX, dataTestY)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=args.batch_size, shuffle=True, sampler=None, batch_sampler=None, num_workers=0)\n",
    "#dataloader_valid = DataLoader(dataset_valid, batch_size=args.batch_size, shuffle=None, sampler=None, batch_sampler=None, num_workers=0)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=args.test_batch_size, shuffle=None, sampler=None, batch_sampler=None, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03529412]]\n",
      "-0.03529411764705882\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "zeros = scaler_sr.transform(np.zeros((1,1)))\n",
    "robust_zero = zeros.item()\n",
    "print(zeros)\n",
    "print(robust_zero)\n",
    "print(scaler_sr.inverse_transform(zeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, look_ahead):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.look_ahead = look_ahead\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "        self.lstm = rnn.LSTM(self.input_size, self.hidden_size, batch_first = True)\n",
    "        self.linear_concat = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, inputs, labels):\n",
    "        _, lstm_hidden = self.lstm(inputs)\n",
    "        lstm_hidden_use = lstm_hidden[0].transpose(1,0)\n",
    "        output = self.linear_concat(lstm_hidden_use)\n",
    "        #print(\"output.shape\", output.shape)\n",
    "        #print(\"labels.shape\", labels.shape)\n",
    "        mse_loss = self.mse(output.squeeze(0), labels)\n",
    "        return output, mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (relu): ReLU()\n",
       "  (mse): MSELoss()\n",
       "  (lstm): LSTM(\n",
       "    (rnn_forward): ModuleList(\n",
       "      (0): LSTMCell(\n",
       "        (fc_xh): Linear(in_features=10, out_features=256, bias=True)\n",
       "        (fc_hh): Linear(in_features=64, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear_concat): Linear(in_features=64, out_features=24, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "bob_worker = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice_worker = sy.VirtualWorker(hook, id=\"alice\")\n",
    "\n",
    "compute_nodes = [bob_worker, alice_worker]\n",
    "\n",
    "# create federated train loader\n",
    "base_dataset = sy.BaseDataset(dataTrainX, dataTrainY)\n",
    "base_federated = base_dataset.federate(compute_nodes)\n",
    "\n",
    "federated_train_loader = sy.FederatedDataLoader(\n",
    "    base_federated,\n",
    "    batch_size = args.batch_size)\n",
    "\n",
    "model_workers = {}\n",
    "model_optims = {}\n",
    "for worker in compute_nodes:\n",
    "    model_workers[worker.id] = LSTM(input_size=data_train.shape[1], hidden_size=args.hidden_dim, \n",
    "            output_size=args.output_dim, look_ahead=args.look_ahead)\n",
    "    model_workers[worker.id] = model_workers[worker.id].to(device)\n",
    "    \n",
    "    model_optims[worker.id] = optim.Adam(model_workers[worker.id].parameters(), lr=args.learning_rate)\n",
    "\n",
    "model_fedavg = LSTM(input_size=data_train.shape[1], hidden_size=args.hidden_dim, \n",
    "            output_size=args.output_dim, look_ahead=args.look_ahead)\n",
    "model_fedavg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(data, target, model, optimizer):\n",
    "    model.send(data.location)\n",
    "    #print(\"current model location :\",model.location)\n",
    "    optimizer.zero_grad()\n",
    "    prediction, loss = model(data, target[:,:,6])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return model.get(), prediction.get(), loss.get()\n",
    "\n",
    "def train():\n",
    "    for worker_id, model in model_workers.items():\n",
    "            model.train()\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader):\n",
    "        loss_got = 0\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        model_workers[data.location.id], _, loss = update(data, target, model_workers[data.location.id], model_optims[data.location.id])\n",
    "        loss_got += loss.item()\n",
    "        #for worker_id, model in model_workers.items():\n",
    "        #    model.get()\n",
    "        model_avg = utils.federated_avg(model_workers)\n",
    "        for worker_id, model in model_workers.items():\n",
    "            model = model_avg.copy()\n",
    "        \n",
    "        if batch_idx % 8 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tBatch loss {:.6f}\\t'.format(\n",
    "                epoch, batch_idx, len(federated_train_loader),\n",
    "                       100. * batch_idx / len(federated_train_loader), loss_got))\n",
    "    return model_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(federated_model):\n",
    "    federated_model.eval()\n",
    "    test_loss = 0\n",
    "    for data, target in dataloader_test:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        prediction, _ = federated_model(data, target[:,:,6])\n",
    "        test_loss += F.mse_loss(prediction.squeeze(0), target[:,:,6], reduction='sum').item()\n",
    "        \n",
    "    test_loss /= len(dataloader_test.dataset)\n",
    "    print('Test set: Average loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number 1\n",
      "Train Epoch: 0 [0/26 (0%)]\tBatch loss 1.100080\t\n",
      "Train Epoch: 0 [8/26 (31%)]\tBatch loss 2.162728\t\n",
      "Train Epoch: 0 [16/26 (62%)]\tBatch loss 0.482679\t\n",
      "Train Epoch: 0 [24/26 (92%)]\tBatch loss 0.982881\t\n",
      "Test set: Average loss: 8.4956\n",
      "Communication time over the network 33.77 s\n",
      "\n",
      "Epoch Number 2\n",
      "Train Epoch: 1 [0/26 (0%)]\tBatch loss 1.027383\t\n",
      "Train Epoch: 1 [8/26 (31%)]\tBatch loss 1.960961\t\n",
      "Train Epoch: 1 [16/26 (62%)]\tBatch loss 0.386804\t\n",
      "Train Epoch: 1 [24/26 (92%)]\tBatch loss 0.679172\t\n",
      "Test set: Average loss: 5.5829\n",
      "Communication time over the network 35.64 s\n",
      "\n",
      "Epoch Number 3\n",
      "Train Epoch: 2 [0/26 (0%)]\tBatch loss 0.641147\t\n",
      "Train Epoch: 2 [8/26 (31%)]\tBatch loss 1.261989\t\n",
      "Train Epoch: 2 [16/26 (62%)]\tBatch loss 0.159777\t\n",
      "Train Epoch: 2 [24/26 (92%)]\tBatch loss 0.220664\t\n",
      "Test set: Average loss: 8.7959\n",
      "Communication time over the network 35.52 s\n",
      "\n",
      "Epoch Number 4\n",
      "Train Epoch: 3 [0/26 (0%)]\tBatch loss 0.295444\t\n",
      "Train Epoch: 3 [8/26 (31%)]\tBatch loss 0.345731\t\n",
      "Train Epoch: 3 [16/26 (62%)]\tBatch loss 0.281083\t\n",
      "Train Epoch: 3 [24/26 (92%)]\tBatch loss 0.167435\t\n",
      "Test set: Average loss: 5.9520\n",
      "Communication time over the network 33.61 s\n",
      "\n",
      "Epoch Number 5\n",
      "Train Epoch: 4 [0/26 (0%)]\tBatch loss 0.246958\t\n",
      "Train Epoch: 4 [8/26 (31%)]\tBatch loss 0.415638\t\n",
      "Train Epoch: 4 [16/26 (62%)]\tBatch loss 0.151763\t\n",
      "Train Epoch: 4 [24/26 (92%)]\tBatch loss 0.167545\t\n",
      "Test set: Average loss: 4.3980\n",
      "Communication time over the network 33.47 s\n",
      "\n",
      "Epoch Number 6\n",
      "Train Epoch: 5 [0/26 (0%)]\tBatch loss 0.242240\t\n",
      "Train Epoch: 5 [8/26 (31%)]\tBatch loss 0.458198\t\n",
      "Train Epoch: 5 [16/26 (62%)]\tBatch loss 0.129659\t\n",
      "Train Epoch: 5 [24/26 (92%)]\tBatch loss 0.165035\t\n",
      "Test set: Average loss: 4.6643\n",
      "Communication time over the network 33.62 s\n",
      "\n",
      "Epoch Number 7\n",
      "Train Epoch: 6 [0/26 (0%)]\tBatch loss 0.240338\t\n",
      "Train Epoch: 6 [8/26 (31%)]\tBatch loss 0.425772\t\n",
      "Train Epoch: 6 [16/26 (62%)]\tBatch loss 0.144799\t\n",
      "Train Epoch: 6 [24/26 (92%)]\tBatch loss 0.164882\t\n",
      "Test set: Average loss: 4.7781\n",
      "Communication time over the network 33.5 s\n",
      "\n",
      "Epoch Number 8\n",
      "Train Epoch: 7 [0/26 (0%)]\tBatch loss 0.240837\t\n",
      "Train Epoch: 7 [8/26 (31%)]\tBatch loss 0.409837\t\n",
      "Train Epoch: 7 [16/26 (62%)]\tBatch loss 0.145202\t\n",
      "Train Epoch: 7 [24/26 (92%)]\tBatch loss 0.166557\t\n",
      "Test set: Average loss: 4.3934\n",
      "Communication time over the network 33.53 s\n",
      "\n",
      "Epoch Number 9\n",
      "Train Epoch: 8 [0/26 (0%)]\tBatch loss 0.241513\t\n",
      "Train Epoch: 8 [8/26 (31%)]\tBatch loss 0.408803\t\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    start_time = time.time()\n",
    "    print(f\"Epoch Number {epoch + 1}\")\n",
    "    federated_model = train()\n",
    "    model_fedavg = federated_model\n",
    "    test(federated_model)\n",
    "    total_time = time.time() - start_time\n",
    "    print('Communication time over the network', round(total_time, 2), 's\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
