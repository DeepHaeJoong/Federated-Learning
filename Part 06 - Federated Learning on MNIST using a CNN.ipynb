{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 06 - Federated Learning on MNIST using a CNN\n",
    "--- \n",
    "\n",
    "### Upgrade to Federated Learning in 10 Lines of PyTorch + PySyft\n",
    "\n",
    "Federated Learning is a very exciting and upsurging Machine Learning technique that aims at building systems that learn on decentralized data. The idea is that the data remains in the hands of its producer (which is also known as the worker), which helps improving privacy and ownership, and the model is shared between workers. One immediate application is for example to predict the next word on your mobile phone when you write text: you don't want the data used for training — i.e. your text messages — to be sent to a central server.\n",
    "\n",
    "The rise of Federated Learning is therefore tightly connected to the spread of data privacy awareness, and the GDPR in EU which enforces data protection since May 2018 has acted as a catalyst. To anticipate on regulation, large actors like Apple or Google have started investing massively in this technology, especially to protect the mobile users' privacy, but they have not made their tools available. At OpenMined, we believe that anyone willing to conduct a Machine Learning project should be able to implement privacy preserving tools with very little effort. We have built tools for encrypting data in a single line as mentioned in our blog post and we now release our Federated Learning framework which leverage the new PyTorch 1.0 version to provide an intuitive interface to building secure and scalable models.\n",
    "\n",
    "In this tutorial, we'll use directly the canonical example of training a CNN on MNIST using PyTorch and show how simple it is to implement Federated Learning with it using our PySyft library. We will go through each part of the example and underline the code which is changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and model specifications\n",
    "\n",
    "First we make the official imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And than those specific to PySyft. In particular we define remote workers alice and bob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy  # <-- NEW: import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")  # <-- NEW: define remote worker bob\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")  # <-- NEW: and alice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the setting of the learning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments(object):\n",
    "    def __init__(self, epochs):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 30\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments(10)\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "## kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "    .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, federated_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        model.send(data.location) # <-- NEW: send the model to the right location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get() # <-- NEW: get the model back\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() # <-- NEW: get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(federated_train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.311331\n",
      "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.212956\n",
      "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 2.020128\n",
      "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 1.562313\n",
      "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 0.907488\n",
      "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.613813\n",
      "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.625688\n",
      "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.526291\n",
      "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.345420\n",
      "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.357533\n",
      "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.494143\n",
      "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 0.481013\n",
      "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.388270\n",
      "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.441536\n",
      "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.277424\n",
      "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.341808\n",
      "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.421876\n",
      "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.116637\n",
      "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.259117\n",
      "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.269566\n",
      "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.143064\n",
      "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 0.320839\n",
      "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.147996\n",
      "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.265410\n",
      "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.098540\n",
      "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.291800\n",
      "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.158989\n",
      "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.317212\n",
      "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.123726\n",
      "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.407062\n",
      "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.211433\n",
      "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.254118\n",
      "\n",
      "Test set: Average loss: 0.2075, Accuracy: 9326/10000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.184428\n",
      "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.062807\n",
      "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.166917\n",
      "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.136897\n",
      "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.087576\n",
      "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.247267\n",
      "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.064453\n",
      "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.059803\n",
      "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.129550\n",
      "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.082901\n",
      "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.055871\n",
      "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.067475\n",
      "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.113296\n",
      "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.099167\n",
      "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.142987\n",
      "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.060608\n",
      "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.459280\n",
      "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.031077\n",
      "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.173692\n",
      "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.106962\n",
      "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.148327\n",
      "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 0.066465\n",
      "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.108965\n",
      "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.065918\n",
      "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.104364\n",
      "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.067583\n",
      "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.101690\n",
      "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.151473\n",
      "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.057934\n",
      "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.185907\n",
      "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.122720\n",
      "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.089118\n",
      "\n",
      "Test set: Average loss: 0.1123, Accuracy: 9673/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.037296\n",
      "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.246267\n",
      "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.049442\n",
      "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.122594\n",
      "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.078722\n",
      "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.148343\n",
      "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.097329\n",
      "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.072736\n",
      "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.143250\n",
      "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.141043\n",
      "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.101288\n",
      "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.021475\n",
      "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.081717\n",
      "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.029653\n",
      "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.092149\n",
      "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.217373\n",
      "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.233567\n",
      "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.039043\n",
      "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.105295\n",
      "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.202249\n",
      "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.076617\n",
      "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.023984\n",
      "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.027892\n",
      "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.048267\n",
      "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.066497\n",
      "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.124659\n",
      "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.123722\n",
      "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.088298\n",
      "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.092719\n",
      "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.046510\n",
      "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.130376\n",
      "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.204969\n",
      "\n",
      "Test set: Average loss: 0.0692, Accuracy: 9798/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.097505\n",
      "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.199664\n",
      "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.106208\n",
      "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.141373\n",
      "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.122769\n",
      "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.064959\n",
      "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.074222\n",
      "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.039030\n",
      "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.127200\n",
      "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.017552\n",
      "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.092273\n",
      "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.089100\n",
      "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.063887\n",
      "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.058203\n",
      "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.027927\n",
      "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.214462\n",
      "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.012347\n",
      "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.029116\n",
      "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.066105\n",
      "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.072757\n",
      "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.072135\n",
      "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.015295\n",
      "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.008823\n",
      "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.101088\n",
      "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.036966\n",
      "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.064305\n",
      "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.129723\n",
      "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.056117\n",
      "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.028032\n",
      "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.014060\n",
      "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.044461\n",
      "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.065906\n",
      "\n",
      "Test set: Average loss: 0.0689, Accuracy: 9770/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.024983\n",
      "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.036043\n",
      "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.096487\n",
      "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.061550\n",
      "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.045168\n",
      "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.052745\n",
      "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.050461\n",
      "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.033467\n",
      "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.089565\n",
      "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.016604\n",
      "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.059723\n",
      "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.102681\n",
      "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.118859\n",
      "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.059456\n",
      "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.021623\n",
      "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.068128\n",
      "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.031518\n",
      "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.017354\n",
      "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.030892\n",
      "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.036232\n",
      "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.027150\n",
      "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.025031\n",
      "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.067567\n",
      "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.090087\n",
      "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.035148\n",
      "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.011774\n",
      "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.047602\n",
      "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.019081\n",
      "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.011985\n",
      "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.162036\n",
      "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.005323\n",
      "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.011710\n",
      "\n",
      "Test set: Average loss: 0.0520, Accuracy: 9844/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.095909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.021511\n",
      "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.009857\n",
      "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.013603\n",
      "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.043174\n",
      "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.061656\n",
      "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.240545\n",
      "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.098197\n",
      "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.119341\n",
      "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.078479\n",
      "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.016309\n",
      "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.013087\n",
      "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.046158\n",
      "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.018044\n",
      "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.022026\n",
      "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.018583\n",
      "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.089551\n",
      "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.011574\n",
      "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.010732\n",
      "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.040827\n",
      "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.082009\n",
      "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.039775\n",
      "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.038233\n",
      "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.043921\n",
      "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.014401\n",
      "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.045865\n",
      "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.035686\n",
      "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.055041\n",
      "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.016135\n",
      "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.134856\n",
      "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.012849\n",
      "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.076703\n",
      "\n",
      "Test set: Average loss: 0.0474, Accuracy: 9847/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.028552\n",
      "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.060836\n",
      "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.147696\n",
      "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.017615\n",
      "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.020444\n",
      "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.165428\n",
      "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.024168\n",
      "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.068033\n",
      "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.004447\n",
      "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.013766\n",
      "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.046259\n",
      "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.032699\n",
      "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.028329\n",
      "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.007506\n",
      "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.041567\n",
      "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.110913\n",
      "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.264679\n",
      "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.088877\n",
      "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.057747\n",
      "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.039149\n",
      "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.085629\n",
      "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 0.037076\n",
      "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.054423\n",
      "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.020969\n",
      "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.020595\n",
      "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.034483\n",
      "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.026728\n",
      "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.016170\n",
      "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.010773\n",
      "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.024572\n",
      "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.042814\n",
      "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.017360\n",
      "\n",
      "Test set: Average loss: 0.0435, Accuracy: 9868/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.007963\n",
      "Train Epoch: 8 [1920/60032 (3%)]\tLoss: 0.041509\n",
      "Train Epoch: 8 [3840/60032 (6%)]\tLoss: 0.104070\n",
      "Train Epoch: 8 [5760/60032 (10%)]\tLoss: 0.063347\n",
      "Train Epoch: 8 [7680/60032 (13%)]\tLoss: 0.004442\n",
      "Train Epoch: 8 [9600/60032 (16%)]\tLoss: 0.005457\n",
      "Train Epoch: 8 [11520/60032 (19%)]\tLoss: 0.021366\n",
      "Train Epoch: 8 [13440/60032 (22%)]\tLoss: 0.042512\n",
      "Train Epoch: 8 [15360/60032 (26%)]\tLoss: 0.031084\n",
      "Train Epoch: 8 [17280/60032 (29%)]\tLoss: 0.008295\n",
      "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.042490\n",
      "Train Epoch: 8 [21120/60032 (35%)]\tLoss: 0.032354\n",
      "Train Epoch: 8 [23040/60032 (38%)]\tLoss: 0.016019\n",
      "Train Epoch: 8 [24960/60032 (42%)]\tLoss: 0.050268\n",
      "Train Epoch: 8 [26880/60032 (45%)]\tLoss: 0.013050\n",
      "Train Epoch: 8 [28800/60032 (48%)]\tLoss: 0.024212\n",
      "Train Epoch: 8 [30720/60032 (51%)]\tLoss: 0.090654\n",
      "Train Epoch: 8 [32640/60032 (54%)]\tLoss: 0.007394\n",
      "Train Epoch: 8 [34560/60032 (58%)]\tLoss: 0.013373\n",
      "Train Epoch: 8 [36480/60032 (61%)]\tLoss: 0.008884\n",
      "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.016127\n",
      "Train Epoch: 8 [40320/60032 (67%)]\tLoss: 0.004758\n",
      "Train Epoch: 8 [42240/60032 (70%)]\tLoss: 0.028498\n",
      "Train Epoch: 8 [44160/60032 (74%)]\tLoss: 0.029244\n",
      "Train Epoch: 8 [46080/60032 (77%)]\tLoss: 0.041897\n",
      "Train Epoch: 8 [48000/60032 (80%)]\tLoss: 0.006371\n",
      "Train Epoch: 8 [49920/60032 (83%)]\tLoss: 0.032963\n",
      "Train Epoch: 8 [51840/60032 (86%)]\tLoss: 0.014700\n",
      "Train Epoch: 8 [53760/60032 (90%)]\tLoss: 0.082580\n",
      "Train Epoch: 8 [55680/60032 (93%)]\tLoss: 0.011285\n",
      "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.025295\n",
      "Train Epoch: 8 [59520/60032 (99%)]\tLoss: 0.081064\n",
      "\n",
      "Test set: Average loss: 0.0430, Accuracy: 9860/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.019809\n",
      "Train Epoch: 9 [1920/60032 (3%)]\tLoss: 0.007519\n",
      "Train Epoch: 9 [3840/60032 (6%)]\tLoss: 0.033462\n",
      "Train Epoch: 9 [5760/60032 (10%)]\tLoss: 0.089832\n",
      "Train Epoch: 9 [7680/60032 (13%)]\tLoss: 0.004217\n",
      "Train Epoch: 9 [9600/60032 (16%)]\tLoss: 0.053010\n",
      "Train Epoch: 9 [11520/60032 (19%)]\tLoss: 0.044002\n",
      "Train Epoch: 9 [13440/60032 (22%)]\tLoss: 0.023941\n",
      "Train Epoch: 9 [15360/60032 (26%)]\tLoss: 0.007477\n",
      "Train Epoch: 9 [17280/60032 (29%)]\tLoss: 0.015219\n",
      "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.020260\n",
      "Train Epoch: 9 [21120/60032 (35%)]\tLoss: 0.090580\n",
      "Train Epoch: 9 [23040/60032 (38%)]\tLoss: 0.139655\n",
      "Train Epoch: 9 [24960/60032 (42%)]\tLoss: 0.021622\n",
      "Train Epoch: 9 [26880/60032 (45%)]\tLoss: 0.036269\n",
      "Train Epoch: 9 [28800/60032 (48%)]\tLoss: 0.030049\n",
      "Train Epoch: 9 [30720/60032 (51%)]\tLoss: 0.026965\n",
      "Train Epoch: 9 [32640/60032 (54%)]\tLoss: 0.011479\n",
      "Train Epoch: 9 [34560/60032 (58%)]\tLoss: 0.022048\n",
      "Train Epoch: 9 [36480/60032 (61%)]\tLoss: 0.041158\n",
      "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.003507\n",
      "Train Epoch: 9 [40320/60032 (67%)]\tLoss: 0.036264\n",
      "Train Epoch: 9 [42240/60032 (70%)]\tLoss: 0.055344\n",
      "Train Epoch: 9 [44160/60032 (74%)]\tLoss: 0.084160\n",
      "Train Epoch: 9 [46080/60032 (77%)]\tLoss: 0.060926\n",
      "Train Epoch: 9 [48000/60032 (80%)]\tLoss: 0.142733\n",
      "Train Epoch: 9 [49920/60032 (83%)]\tLoss: 0.003990\n",
      "Train Epoch: 9 [51840/60032 (86%)]\tLoss: 0.019390\n",
      "Train Epoch: 9 [53760/60032 (90%)]\tLoss: 0.061034\n",
      "Train Epoch: 9 [55680/60032 (93%)]\tLoss: 0.053874\n",
      "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.012969\n",
      "Train Epoch: 9 [59520/60032 (99%)]\tLoss: 0.131266\n",
      "\n",
      "Test set: Average loss: 0.0414, Accuracy: 9861/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.011702\n",
      "Train Epoch: 10 [1920/60032 (3%)]\tLoss: 0.017724\n",
      "Train Epoch: 10 [3840/60032 (6%)]\tLoss: 0.087545\n",
      "Train Epoch: 10 [5760/60032 (10%)]\tLoss: 0.008798\n",
      "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.018757\n",
      "Train Epoch: 10 [9600/60032 (16%)]\tLoss: 0.017461\n",
      "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.006788\n",
      "Train Epoch: 10 [13440/60032 (22%)]\tLoss: 0.016449\n",
      "Train Epoch: 10 [15360/60032 (26%)]\tLoss: 0.116836\n",
      "Train Epoch: 10 [17280/60032 (29%)]\tLoss: 0.005375\n",
      "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.005351\n",
      "Train Epoch: 10 [21120/60032 (35%)]\tLoss: 0.016059\n",
      "Train Epoch: 10 [23040/60032 (38%)]\tLoss: 0.010716\n",
      "Train Epoch: 10 [24960/60032 (42%)]\tLoss: 0.008441\n",
      "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.038296\n",
      "Train Epoch: 10 [28800/60032 (48%)]\tLoss: 0.015305\n",
      "Train Epoch: 10 [30720/60032 (51%)]\tLoss: 0.047844\n",
      "Train Epoch: 10 [32640/60032 (54%)]\tLoss: 0.025396\n",
      "Train Epoch: 10 [34560/60032 (58%)]\tLoss: 0.008374\n",
      "Train Epoch: 10 [36480/60032 (61%)]\tLoss: 0.021849\n",
      "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.021767\n",
      "Train Epoch: 10 [40320/60032 (67%)]\tLoss: 0.017802\n",
      "Train Epoch: 10 [42240/60032 (70%)]\tLoss: 0.056274\n",
      "Train Epoch: 10 [44160/60032 (74%)]\tLoss: 0.021304\n",
      "Train Epoch: 10 [46080/60032 (77%)]\tLoss: 0.014214\n",
      "Train Epoch: 10 [48000/60032 (80%)]\tLoss: 0.007487\n",
      "Train Epoch: 10 [49920/60032 (83%)]\tLoss: 0.117058\n",
      "Train Epoch: 10 [51840/60032 (86%)]\tLoss: 0.031149\n",
      "Train Epoch: 10 [53760/60032 (90%)]\tLoss: 0.009620\n",
      "Train Epoch: 10 [55680/60032 (93%)]\tLoss: 0.005946\n",
      "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.054352\n",
      "Train Epoch: 10 [59520/60032 (99%)]\tLoss: 0.015818\n",
      "\n",
      "Test set: Average loss: 0.0425, Accuracy: 9871/10000 (99%)\n",
      "\n",
      "CPU times: user 14min 41s, sys: 5.24 s, total: 14min 46s\n",
      "Wall time: 14min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
